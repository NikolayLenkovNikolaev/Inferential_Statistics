{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqVlQPapet9XzbHj25ttSs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikolayLenkovNikolaev/Inferential_Statistics/blob/main/4_Multiple_Random_Variables.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Joint and Marginal Distribution\n",
        "\n",
        "- multivariate models\n",
        "\n",
        "4.1.1. An n-dimensional random vector is a function from a sample space S into $R^n$, n-dimensional Euclidian space\n",
        "\n",
        "ex. 4.1.2. Sample space for dice\n",
        "\n",
        "4.1.3. Let (X, Y) be a discrete bivariate random vector. Then the function f(x, y) from $R^n$ into $R$ defined by f(x,y)= P(X=x; Y=y) is called the joint probability mass function or joint pmf of (X, Y). If it is necessary to stress the fact that f is the join pmf of the vector (X, Y) rather than some other vector, the notation $F_{X.Y} (x, y) will be used$\n",
        "\n",
        "ex.4.1.4 continuation of exmple 4.1.2.\n",
        "\n",
        "ex 4.1.5. Joint pmf for dice\n",
        "\n",
        "Theorem 4.1.6. Let (X, Y) be a discrete bivariate random vector with joint pmf $f_{X,Y} (x, y)$. Then the marginal pmf-s of X and Y, $f_X(x)= P(X=x)$ and $f_Y (y)= P(Y=y)$, are given by\n",
        "\n",
        "$f_X(x)= \\sum_{y \\in R} f_{X,Y} (x,y)$ and $f_Y (y)= \\sum_{x \\in R} f_{X,Y} (x,y)$\n",
        "\n",
        "Example 4.1.7. Marginal pmf for dice\n",
        "\n",
        "Example 4.1.8. Dice probability\n",
        "\n",
        "Example 4.1.9. Same marginals, different joint pmf\n",
        "\n",
        "4.1.10 A function $f(x,y)$ from $R^n$ intp $R$ is called a joint probability density function or joint pdf of the continuous bivariate random vector (X, Y) if, for every $A \\subset R^n$\n",
        "\n",
        "$$P((X,Y) \\in A) = \\int_A \\int f(x,y) dx dy$$\n",
        "\n",
        "Ex.4.1.11. and 4.1.12 Calculating joint probabilities\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5T1uOq1VNsmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. Conditional Distribution and Independence\n",
        "\n",
        "4.2.1. Let (X,Y) be a discrete bivariate random vector with joint pmf $f(x, y)$ and marginal pmf-s $f_X(x)$ and $f_Y(y)$. For any x such that $P(X=x) = f_X(x) > 0$ , the conditional pmf of Y given that X=x is the function of y denoted by $f(y|x)$ and defined by:\n",
        "\n",
        "$$f(y|x) = P(Y=y|X=x) = \\frac{f(x,y)}{f_X(x)}$$\n",
        "\n",
        "\n",
        "ex. 4.2.2. Calulating conditional probabilities\n",
        "\n",
        "4.2.3. Let (X,Y) be a continuous bivariate random vector with joint pdf f(x,y) and marignal pdfs $f_X(x)$ and $f_Y(y)$. For any x such that $f_X(x)> 0$, the conditional pdf of Y given that X=x is the dunction of y denoted by f(y|x) and defined by ⁉\n",
        "$$f(y|x)= \\frac{f(x,y)}{f_X(x)}$$\n",
        "\n",
        "\n",
        "ex. 4.2.4 - calculating conditional pdf-s\n",
        "\n",
        "4.2.5. Let (X,Y) be a bivariate random vbector with joint pdf or pmf f(x,y) and marginal pdfs or pmfs $F_X(x)$ and $f_Y(y)$. Then X and Y are called independent random variables if , for every $x \\in R$ and $y \\in R$\n",
        "\n",
        "$$f(x,y) = f_X(x) f_Y(y)$$\n",
        "\n",
        "\n",
        "if....\n",
        "\n",
        "\n",
        "Ex: 4.2.6. Checking independence\n",
        "\n",
        "Lemma 4.2.7. Let (X,Y) be a bivariate random vector wit joint pdf or pmf $f(x,y)$. Then X and Y are independent random variables if and only if there exist functions $g(x)$ and $h(y)$ such that, for every $x \\in R$ and $y \\in R$\n",
        "\n",
        "$$f(x,y) = g(x) h(x)$$\n",
        "\n",
        "ex: 4.2.8. Checking independence\n",
        "\n",
        "ex. 4.2.9. Joint probability model\n",
        "\n",
        "Theorem: 4.2.10: let X and Y be independent random variables:\n",
        "a. for any $A \\subset R$ and $B \\subset R$ , $P(X \\in A , Y \\in B)= P(A \\in A)P(Y \\in B)$ that is, the events [X \\in A] and [Y \\in B] are independent events.\n",
        "\n",
        "b. Let $g(x)$ be a function only of x and h(y) be a function only of y. Then\n",
        "$$E[g(X)h(Y)] = (E[g(X)])(E[h(Y)])$$\n",
        "\n",
        "proof:\n",
        "\n",
        "ex: 4.2.11. Expectations of independent variables\n",
        "\n",
        "\n",
        "Theorem: 4.2.12. Let X and Y be independent random variables with moment generating functions $M_X(t)$ and $M_Y(t)$. Then the moment generating function of the random variables Z=X+Y i s given by : $$M_Z (t)= M_X(t) M_Y(t)$$\n",
        "\n",
        "ex.4.2.13. Mgf of a sum of normal variables\n",
        "\n",
        "Theorem 4.2.14: Let $X \\sim N(\\mu, \\sigma^2)$ and $Y \\sim N(\\gamma; \\tau^2)$ be independent noraml rando variables. Then the random variables Z=X+Y has a $N(\\mu + \\gamma; \\sigma^2 + \\tau^2)$ distribution\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8TdwE-1JNsox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3. Bivariate Transformatioons\n",
        "\n",
        "ex\" 4.3.1. Distribution of the sum of Poisson variables. Let X and Y be independent  Poisson random variables with parameter ....\n",
        "\n",
        "Theorem: 4.3.2. If $X \\sim Poisson(\\theta)$ and $Y \\sim Poisson(\\gamma)$ and X and Y are independent, then X+Y $\\sim Poisson (\\theta + \\gamma)$\n",
        "\n",
        "Ex: 4.3. Distribution of the product of beta variables\n",
        "\n",
        "ex: 4,3,4, Sun and difference of normal variables\n",
        "\n",
        "TYheorem: 4.3.5. Let X and Y be independent random variables. Let $g(x)$ be a function only of x and h(y) ne a function only of y. Then the random variables $U=g(X)$ and $V=h(Y)$ are indepdent\n",
        "\n",
        "Proof\n",
        "\n",
        "Ex: 4.3.6. Distribution of the ratio of normal variables\n",
        "\n"
      ],
      "metadata": {
        "id": "ZOQhU6aaNsr0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "y9KDXEN2NsuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4. Hierarchical Models and Mixture DIStributions\n",
        "\n",
        "Ex: 4.4.1. Binomila-Poisson hierarchy\n",
        "\n",
        "Ex:  4.4.2. Contnuation of ex.4.4.1.\n",
        "\n",
        "\n",
        "Theorem: 4.4.3. If X and Y are any two random variables, then E[X] = E[E(X|Y)] rpovided that the expectations exist.\n",
        "\n",
        "Proof:\n",
        "\n",
        "4.4.4. A random variables X is said to have a mixture distribution if the distribution of X depends on a quantity that also has a distribution\n",
        "\n",
        "Ex: 4.4.5. Generalization of Ex.4.4.1\n",
        "\n",
        "Ex: 4.4.6 Beta-binomial hierarchy\n",
        "\n",
        "Theorem: 4.4.7. COnditioanl variance identity: for any wo random variables X and y:\n",
        "\n",
        "$$Var[X] = E[Var(X|Y)] + Var(E[X|Y])$$\n",
        "prodided that the expectations exist\n",
        "\n",
        "proof:\n",
        "\n",
        "Ex: 4,4,8: continuos of example 4.4.6\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G1CKa4tGNswW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5. Covariance and COrrelation\n",
        "\n",
        "4.5.1. The covariance of X and Y is the number defined by:\n",
        "$$Cov(X,Y) = E((X- \\mu_X)(Y - \\mu_Y))$$\n",
        "\n",
        "4.5.2. TYhe correlation of X and Y is the number defined by ⁉\n",
        "$$\\rho_{X,Y} = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}$$\n",
        "\n",
        "- The value $\\rho_{X,Y}$ is also called the correlation coefficient.\n",
        "\n",
        "Theorem: 4.5.3. For any random variables X and Y\n",
        "$$Cov(X,Y)= E[XY]- \\mu_x \\mu_y$$\n",
        "\n",
        "proof\n",
        "\n",
        "Ex: 4.5.4. correlation\n",
        "\n",
        "Theorem: 4.5.5. If X and Y are independent random variables, ten $Cov(X, Y)= 0$ and $\\rho_{X,Y}= 0$\n",
        "\n",
        "prof\n",
        "\n",
        "Theorem: 4.5.6. If X na dY are any two random variables and a and b are any two constant, then\n",
        "\n",
        "$$Var[aX + bY] = a^2 Var[X] + b^2 Var[Y] + 2ab. Cov(X,Y)$$\n",
        "\n",
        "If X and Y are independent random variables , then\n",
        "\n",
        "$Var(aX + bY) = a^2 Var[X] + b^2.Var[Y]$\n",
        "\n",
        "proof:\n",
        "\n",
        "Theorem: 4.5.7. For any random variables X and Y\n",
        "1. $-1 =< \\rho_{X,Y} =< 1$\n",
        "2. $|\\rho_{X,Y}|=1$ if and only if there exist numbers $a \\ne 0$ and b uch that $P(Y=aX + b)=1$. If $\\rho_{X,Y}=1$ then $a>0$ and if $\\rho_{X,Y}=-1$ then a<0.\n",
        "\n",
        "proof:\n",
        "\n",
        "Example: 4.5.8. Correlation\n",
        "\n",
        "Def 4.5.10. Let $-\\infty < \\mu_X < \\infty$, $-\\infty < \\mu_Y < \\infty$, $0< \\sigma_X, 0< \\sigma_Y$ and $-1 < \\rho < 1$ be a five real numbers. The bivariate normal pdf with means $\\mu_X, \\mu_Y$, variance $\\sigma_X^2, \\sigma_Y^2, \\rho$ is the bivariate pdf given by:\n",
        "\n",
        ".....................\n",
        "\n",
        "for $-\\infty < x < \\infty$ and $-\\infty < y < \\infty$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lASU4D83Nsy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6. Multivariate DIstributions\n",
        "\n",
        "def: 4.6.2. Let n and m be positive integers and let $p_1...p_n$ be numbers satisfying $0 =< p_i =< 1$ i=1...n and $\\sum_{i=1}^n p_i = 1$. Then the random vector $(X_1, ... X_n)$ has a multinomial distribution with m trials and cell probabilities $p_1...p_n$ if the join pmd of ($X_1...X_n$) is\n",
        "\n",
        "$f(x_1...x_n) = \\frac{m!}{x_1!...x_n!}p_1^{x_1}.....p_n^{x_n} = m~ ∏_{i=1}^n \\frac{p_i^{x_i}}{x_i!}$\n",
        "\n",
        "on the set (x_1...x_n) such that each $x_i$ is a nonegative integer and $\\sum_{i=1}^n x_i = m$\n",
        "\n",
        "Ex\" 4.6.4. Multivariate pmf\n",
        "\n",
        "Theorem: 4.6.4. Multinomial Theorem:\n",
        "\n",
        "Let m and n be positive integers. Let A be the set of vectors $\\vec{x}=(x_1...x_n)$ such that each $x_i$ is a nonegative integer and $\\sum_{i=1}^n x_i = m$. Then , for any real numbers $p_1...p_n$\n",
        "\n",
        "$$(p_1+...+p_n)^m = \\sum_{x \\in A} \\frac{m!}{x_1! ... x_n!} p_1^{x_1}.....p_n^{x_n}$$\n",
        "\n",
        "4.6.5. Let $X_1...X_n$ be random vectors with joint pdf or pmf $f(x_1...x_n)$. Let $f_X_i (x_i$)  denote the marginal pdf or pmf of $X_i$. Then $X_1...X_n$ are called mutually independent random vectors if, for every $(x_1..x_n)$\n",
        "\n",
        "$$f(x_1...x_n) = f_{X_1}(x_1) ... f_{X_n}(X_n) = \\prod_{i=1}^n f_{X_i} (x_i)$$\n",
        "\n",
        "Theorem: 4.6.6. Generalization of Theorem 4.2.10. Let $X_1...X_n$ be mutually independent random variables with mgfs $M_{X_1} (t) ... M_{X_n} (t)$. Let $Z= C_1 + ...+ X_n$. Then the mgf of Z is:\n",
        "\n",
        "$$M_Z (t) = M_{X_1}(t) ... M_{X_n} (t)$$\n",
        "\n",
        "In particular if $X_1...X_n$ all ahave the same distribtion with mgf $M_X(t)$ ,then $$M_Z(t)= (M_X(t))^n$$\n",
        "\n",
        "ex\" 4.6.8. Mgf of a sum of gamma variables\n",
        "\n",
        "Corollary 4.6.9\" Let $X_1...X_n$ be mutually independent random variables with mgfs $M_{X_1}(t) ... , M_{X_n}(t)$. Let $a_1...a_n$ and $b_1 ... b_n$ be fixed constants. Let $Z= (a_1.X_1 + b_1 + ... + (a_n X_n + b_n))$. Then the mgf of Z is:\n",
        "\n",
        "$$M_Z(t)= (e^{t[\\sum b_i]}) M_{X_1} (a_1.t) ... M_{X_n} (a_n t)$$\n",
        "\n",
        "Continua:////////////////\n",
        "\n"
      ],
      "metadata": {
        "id": "NuLgCVxvdHn3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.7. Inequalitites\n",
        "###  Numerical\n",
        "\n",
        "Lemma 4.7.1. Le a nad b be any positive numbers , and let p and q be any positive number (necessarily greater than 1) satisfying\n",
        "\n",
        "$$\\frac{1}{p} + \\frac{1}{q} = 1$$\n",
        "Then:\n",
        "$$ \\frac{1}{p} a^p + \\frac{1}{q} b^q >= ab$$\n",
        "\n",
        "with equality if and only if $a^p = b^q$\n",
        "\n",
        "Proof:\n",
        "\n",
        "Theorem: 4.7.2.: Holder's Inequality: Let X and Y be any two random variables and let p and q satisfy 4.7.1. Thne:\n",
        "\n",
        "$$|E[XY]| =< E[|XY|] =, (E[|X|^p])^{1/p} (E[|Y|^q])^{1/q}$$\n",
        "\n",
        "Proof:\n",
        "\n",
        "\n",
        "Theorem: 4.7.3. Cauchy - Schwarz Inequality: for any two random varibales X and Y:\n",
        "$$|E[XY]| =< E[|XY|] =< (E[|X|^2])^{1/2}  (E[|Y|^2])^{1/2} $$\n",
        "\n",
        "ex.4.7.4.\n",
        "\n",
        "Theorem: 4.7.5. Minkowski Inequality:....\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Functional\n",
        "\n",
        "4.7.6. A function $g(x)$ is convex if $g(\\lambda x + (1-\\lambda)y) =< \\lambda g(x)  + (1-\\lambda)g(y)$ for all x and y, and $0 < \\lambda < 1$. The function g(x) is concave if $-g(x)$ is convex.\n",
        "\n",
        "Theorem. 4.7.7. Jensen's Inequality: For any random variable X if g(x) is a convex function, then:\n",
        "\n",
        "$$E[g(X)] >= g(E[X])$$\n",
        "\n",
        "Equalioty hold if and only if, for every line a + bx that is tangent to g(x) at x = E[X], P(g(X)= a + bX) =1\n",
        "\n",
        "Proof:\n",
        "\n",
        "Ex: 4.7.8. An inequality for means\n",
        "\n"
      ],
      "metadata": {
        "id": "8aue36IndHql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HWthYxrddHtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JbOlYmDsdHvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VqnO-oG0dHya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZhuasGmadH0h"
      }
    }
  ]
}