{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWLGem52VEemI8oYKSkI03",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikolayLenkovNikolaev/Inferential_Statistics/blob/main/2_Transformation_and_Expectations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Distribution of Function of a Random Variables\n",
        "\n",
        "Theorem: 2.1.3. Let X have cdf $F_X (x)$, let $ =g(X)$, and let X` abd Y` be defined as in  2.1.7.\n",
        "a. If g is an increasing function of X`, $F_Y(y)= F_X(g^{-1}(y))$ for  $y \\in Y`$\n",
        "\n",
        "b. If g is a decreasing function on X` and X is a continuous random variable, $F_Y (y)= 1-F_X(g^{-1}(y))$ for $y \\in Y`$\n",
        "\n",
        "Theorem: 2.1.15: Let X have pdf $f_X(x)$ and let $Y=g(X)$, where g is a monotone function. Let X` and Y` be defined by 2.1.7. Suppose that $f_X(x)$ is a continuous on X` and that $g^{-1}(y)$ has a continuous derivative on Y`. Then the pdf of Y is given by:\n",
        "\n",
        "$$f_Y(y)=> $$\n",
        "- $f_X(g^{-1}(y))| \\frac{d}{dy} g^{-1} (y) | y \\in Y`$\n",
        "- 0 - otherwise\n",
        "\n",
        "\n",
        "\n",
        "Theorem 2.1.10. Probability integral transformation: Let X have continuous cdf F_X(x) and define the random variable Y as $Y=F_X(X)$. Then Y is uniformly distribution on (0,1), that is $P(Y =< y) = y$, $0<y<1$\n",
        "\n",
        "Proof:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "djhs53-NMVRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Expected Values\n",
        "\n",
        "2.2.1. The aexpected value or mean of a random variable $g(X)$ , denoted by E[g(X)] is:\n",
        "- if X is continuous: $\\int_{\\infty}^{\\infty} g(x) f_X(x) dx$\n",
        "- if X is discrete: $\\sum_{X \\in X`} g(x) f_X(x)= \\sum_{x \\in X`} g(x) P(X=x)$\n",
        "\n",
        "Theorem: 2.2.5.\n",
        "\n",
        "Let X be a random variable and let a,b cna c ce a contants. Then any function $g_1(x), g_2(x)$  whose expectations exist.\n",
        "\n",
        "- $E[a.g_1(X) + b.g_2 (X) +c] = a.E[g_1(X)] + b.E[g_2(X)]+ c$\n",
        "- if $g_1(x) >= 0$ for all x , then $E[g_1(X)] >= 0$\n",
        "- if $g_1(x) >= g_2(x)$  for all x, then E[g_1(X)] >=- E[g_2(X)]\n",
        "- if $a =< g_1(x) =< b for all x, then a =< E[g_1(X)] =< b$\n",
        "\n"
      ],
      "metadata": {
        "id": "010bCat7MVUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gk8qxD-CMVW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Moments and Moment Generating Functions\n",
        "\n",
        "2.3.1. For each integer n, then n-th moment of X (or $F_X(x)$), $\\mu`_n$ is $$\\mu`_n = E[X^n]$$\n",
        "\n",
        "The n-the cetral moment of X , $\\mu_n$ is:\n",
        "$$\\mu_n = E[(X -\\mu)^n]$$\n",
        "\n",
        "- where $\\mu = \\mu`_1 = E[X]$\n",
        "\n",
        "2.3.2. The variance of a random variable X is its second central moment $Var[X]= E[X-e[X]]^2$. The positive square root of Var[X] us the standard deviation of X\n",
        "\n",
        "Theorem: 2.3.4. If X is a ranodm variable with finite variance , then any constants a and b\n",
        "\n",
        "$$Var{aX+b} = a^2. Var[X]$$\n",
        "\n",
        "2.3.6. Let X be a random variable with cdf $F_x$. The moment generatinf function MGF  of X  (or F_X) denoted by $M_X(t)$ is:\n",
        "\n",
        "$$M_X(t)= E[e^{t.X}]$$\n",
        ", provided that the expectation exists for t in some neighborhood of 0. That is, there is an $h>0$ such that, for all t in $-h < t < t$, $E[e^{tX}]$ exist. If the expectation does not exist in a neighborhood of 0, we say that the moment generating function does not exist.\n",
        "\n",
        "- if X continuos: $\\int_{-\\infty}^{\\infty} e^{tx} f_X (x) dx$\n",
        "- if X discrete: $\\sum_x e^{tx} P(X=x)$\n",
        "\n",
        "\n",
        "Theorem: 2.3.7. If X has mgf $M_X(t)$, then\n",
        "- $E[X_n]= M_X^{(n)}(0)$\n",
        "\n",
        "where we define\n",
        "$$M_X^{(n)} (0) = \\frac{d^n}{dt^n} M_X(t)|_{t=0}$$\n",
        "\n",
        "That is, the n-th moment is equal to the n-th derivative of $M_X(t)$ evaluated at $t=0$\n",
        "\n",
        "Proof:\n",
        "\n",
        "ex. 2.3.8. Gamma mgf\n",
        "\n",
        "ex. 2.3.9. Binomial mgf\n",
        "\n",
        "ex. 2.3.10. Nonunique moment\n",
        "\n",
        "Theorem: 2.3.11. Let $F_X(x)$ and $F_Y(y)$ be two cdf-s all of whose moments exist\n",
        "1. if  A and Y have bounded support, then $F_X(u) = F_Y(u)$ for all u if and only if $E[X^r] = E[Y^r]$ for all integers r=0,1,2,...\n",
        "2. If the moment generating functions exist and $M_X(t)= M_Y(t)$ for all t in some neighborhood of 0, then $F_X(u) = F_Y(u)$ for all u\n",
        "\n",
        "Theorem: 2.3.13 - COnvergence of mgf-s\n",
        "SUppose $X_i, i =1,2...$ is a sequence of random variable ,e ach with mgf $M_{x_i} (t)$. Furthermore, suppose that:\n",
        "$lim_{i->\\infty} M_{X_i} (t)= M_x(t)$ - for all t in a neighborhood of 0\n",
        "\n",
        "- and $M_X(t)$ is an mgf. Then there is a unique cdf $F_X$ whose moments are determined by $M_X(t)$ and , for all x where $F_X(x)$ is continuous , we have:\n",
        "$lim_{i=\\infty} F_{X_i} (x)= F_X(x)$\n",
        "\n",
        "That is, convergence , for $|t| < h$, of mgf-s to an mgf implies convergence of cdf-s\n",
        "\n",
        "Ex: 2.3.13. Poisson approximation\n",
        "\n",
        "Lemma: 2.3.14. Let $a_1,a_2...$ be a sequence of numbers converging to a , that is, $lim_{n->\\infty} a_n = a$. Then:\n",
        "\n",
        "$$lim_{n-> \\infty} (1 + \\frac{a_n}{n})^n e^a$$\n",
        "\n",
        "\n",
        "Theorem: 2.3.15. For any constantsa and b, the mgf of the random variable aX + b  is given by: $M_{aX+b} (t) = e^{bt} M_X(at)$\n",
        "\n",
        "Proof:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WNSsbpaOMVZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4. Differenctiating under an Integral Sign\n",
        "\n",
        "Theorem 2.4.1. Leibniz's Rule\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ScLnOXD_MVcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zFMPHey2MVfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1n9tEsP7MVh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "P82w9yyOMVki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9rcQ4tlVMVm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Uvcye0A2MVpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qbehfthCMVsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "flhH-K8BMVu8"
      }
    }
  ]
}