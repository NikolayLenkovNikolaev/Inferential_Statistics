{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/Gt8YeYScaBAH3U7I61E8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikolayLenkovNikolaev/Inferential_Statistics/blob/main/5_Random_Sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Concept of Random Sample\n",
        "\n",
        "5.1.1. The random variables $X_1..X_n$ ARE CALLED A RANDOM SAMPLE OF SIZE N FROM THE POPULATION $F(x)$ IF $X_1... X_n$ are mutually independent random variableas and marginal pdf or pmf of each $X_i$ is the same function $f(x)$. ALternatively, $X_1 ... X_n$ are called independent and identically distributited random variables with pdf or pmf $f(x)$. This is commonly abbreviated to iid random variables.\n",
        "\n",
        "\n",
        "$$f(x_1 ...x_n| \\theta) = \\prod_{i=1}^n$ f(x_i|\\theta)$\n",
        "\n",
        "ex: 5.1.2. Sample pdf-exponential\n",
        "\n",
        "ex 5.1.3. Finite population model;\n",
        "\n"
      ],
      "metadata": {
        "id": "xYXmqOOYOMzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2. Sums of Random Variables from a Random Sample\n",
        "\n",
        "5.2.1. Let $X_1 ...X_n$ be a random sample of size n from a population and let $T(x_1 ...x_n)$ be a real-valued or vector-valued function whose domain includes the sample space of $(X_1 ...X_n)$. Then the random variable or random vector $Y=T(X_1 ..X_n)$ is called a statistic. The probability distribution of a statistic Y is called the sampling distribution of Y.\n",
        "\n",
        "5.2.2. The sample mean is the aritmentic average of the values in a random sample. It is usually denoted by:\n",
        "$$\\bar{X} = \\frac{X_1 + ...+ X_n}{n} = \\frac{1}{n} \\sum_{i=1}^n X_i$$\n",
        "\n",
        "5.2.3. The sample variance is the satistic defined by:\n",
        "$$S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2$$\n",
        "\n",
        "The sample standard deviation is the statistic defined by $S= \\sqrt{S^2}$\n",
        "\n",
        "Theorem:\" 5.2.4. Let $x_1...x_n$ be any number and $\\bar{x} = (x_1 + ...+ x_n)/n$. Then:\n",
        "1. $min_a \\sum_{i=1}^n (x_i - a)^a = \\sum_{i=1}^n (x_i - \\bar{x})^2$\n",
        "2. $(n-1)s^2 = \\sum_{i=1}^n (x_i - \\bar{x})^2  = \\sum_{i=1}^n x^2- n.\\bar{x}^2$\n",
        "\n",
        "Proof:\n",
        "\n",
        "Lemma 5.2.5. Let $X_1 ... X_n$ be a random sample from a population and let g(x) be a function such that $E[g(X_1)]$ and $Var[g(X_1)]$ exist. Then:\n",
        "$$E(\\sum_{i=1}^n g(X_i)) = n(E[g(X_1)])$$\n",
        "\n",
        "and\n",
        "\n",
        "$$Var(\\sum_{i=1}^n g(X_i)) = n(Var(g(X_1)))$$\n",
        "\n",
        "Proof:\n",
        "\n",
        "Theorem: 5.2.6. Let $X_1.. X_n$ be a random sample from a population with mean $\\mu$ and variance $\\sigma^2 < \\infty$. Then:\n",
        "\n",
        "- $E[\\bar{X}]= \\mu$\n",
        "- $ Var(\\bar{X}) = \\frac{\\sigma^2}{n}$\n",
        "- E[S^2] = \\sigma^2$\n",
        "\n",
        "Proof:\n",
        "\n",
        "Theorem: 5.2.7. Let $X_1 ... X_n$ be a random sample from a population with pdf $M_X(t)$. Then the mgf of the sample mean is:\n",
        "$$M_{\\bar{X}}(t)= [M_X(t/n)]^n$$\n",
        "\n",
        "ex: 5.2.8. Distriution of the mean\n",
        "\n",
        "Theorem: 5.2.9. If X and Y are independent continuous random variable with pdfs $f_X(x)$ and $f_Y(y)$ , then the pdf of Z=X+Y is:\n",
        "$$f_Z = \\int_{-\\infty}^{\\infty} = f_{X,Y} (w, z-w) = f_X (w) f_Y(z-w)$$\n",
        "Integratting out w, we obtain the marginal pdf of Z as given 5.2.3.\n",
        "\n",
        "Ex: 5.2.10 Sum of Cauchy random variables\n",
        "\n",
        "Theorem: 5.2.11. Suppose $X_1...X_n$ is a random sample from a pdf or pmf $f(x|\\theta)$\n",
        " where\n",
        "\n",
        " $$f(x|\\theta) = h(x) c(\\theta) exp(\\sum_{i=1}^k w_i(\\theta) t_i(x))$$\n",
        "\n",
        " is a member of an exponential family. Define statistics $T_1..T_k$ by\n",
        "\n",
        " $$T_i*(X_1...X_n = \\sum_{j=1}^n t_i (X_j)$$\n",
        " - i=1...k\n",
        " - If the set {$(w_1(\\theta), w_2(\\theta)...w_k(\\theta), \\theta \\in \\Theta)$} cotains an open subset of $R^k$, then the distribution of $T_1...T_k$ is an exponential family of the form:\n",
        "\n",
        " $$f_T(u_1 ...u_k|\\theta) = H(u_1 ...u_k)[c(\\theta)]^n exp(\\sum_{i=1}^k w_i(\\theta) u_i$$\n",
        "\n",
        "\n",
        "Ex: 5.2.12. Sum of Bernoulli random variable\n",
        "\n"
      ],
      "metadata": {
        "id": "kJpjXgD7OM16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 5.3. Sampling from the Normal Distribution\n",
        "\n",
        "### 5.3.1. Properties of the Sample Mean and Variance:\n",
        "\n",
        "Theorem 5.3.1. Let $X_1 ...X_n$ be a random sample from a $N(\\mu, \\sigma^2)$ distribution, and let $X=(1/n) \\sum_{i=1}^n$ and $S^2 = [1/(n-1)]\\sum_{i=1}^n (X_i - \\bar{X})^2$ $$. Then:\n",
        "- $\\bar{X}$ and $S^2$ are independent random variables\n",
        "- $\\bar{X}$ has a $N(\\mu, \\sigma^2 /n)$ distribution\n",
        "- $(n-1)S^2/\\simga^2$ has a chi-squared distribution with n-1 degrees of freedom\n",
        "\n",
        "Proof:\n",
        "\n",
        "Lemma: 5.3.2. Facts about chi squared random variables: We use the notation $\\chi_p^2$ to denote a chi squared random variable with p=degree of freedom\n",
        "- if Z is a $N(0,1)$ random variable, then $X^2 \\sim \\chi_1^2$, that is , the square of a standard normal random variable is a chi squared random variable.\n",
        "\n",
        "- if $X_1 ...X_n$ are independet na d$X_i \\sim \\chi_p^2$, then $X_1 ... + X_n \\sim \\chi_{p_1 + ... p_n}$, that is, indepdndent chi squared variable add to a chi squared variable and the degrees of freedom also add\n",
        "\n",
        "Proof\n",
        "\n",
        "\n",
        "Proof theorem 5.3.1.\n",
        "\n",
        "Lemma 5.3.3. Let $X_j \\sim N(\\mu_j, \\sigma_j)$ j=1,...n independent. For constants $a_{ij}$ and $b_{rj}(j=1 ...m, i=1...k, r=1...m)$ where $k+m =< n$ deifne\n",
        "\n",
        "$$U_i = \\sum_{j=1}^n a_{ij} X_j ---- i=1...k$$\n",
        "\n",
        "$$ V_r = \\sum_{j=1}^n b_{rj} X_j; ------r=1...m$$\n",
        "\n",
        "a. the random variables $U_i$ and $Var_r$ are independent if and only if $Cov(U_i, V_r) = 0$. Furthermore $Cov(U_i, V_r) = 'sum_{j=1}^n a_{ij} b_{rj} \\sigma_j^2}$\n",
        "b. The random vectors $(U_1 ...U_k)$ and $(V_1 .. V_m)$ are independent if and only if $U_i$ is independent of $V_r$ for all pairs $i, r (i=1...k ; r=1...m)$\n",
        "\n",
        "Proof:\n",
        "\n",
        "### 5.3.2. The  Derived Distributios: Student's and Snedecor's F\n",
        "5.3.4. Let $X_1 ..X_n$ be a random sample from a $N(\\mu, \\sigma^2)$  distribution. The quantity $(X_\\mu)/ (S/\\sqrt{n})$ has Student't distribution with n-1 degrees of freedom. Equivalently, a random variable T has Student's distribution with p-degrees of freedom and we write $T \\sim t_p$ if it has pdf:\n",
        "\n",
        "$$f_T(t)= \\frac{\\Gamma(\\frac{p-1}{2})}{\\Gamma(\\frac{p}{2})} \\frac{1}{(p.\\pi)^{1/2}} \\frac{1}{(1+t^2/p)^{(p+1)/2}}$$\n",
        "\n",
        "Ex 5.3.5. Variance ratio distribution\n",
        "\n",
        "\n",
        "5.3.6. Let $X_1...X_n$ be a random sample from a $N(\\mu_X; \\sigma_X^2)$ population, and let $Y_1...Y_m$ be a random sample from an independent $N(\\mu_Y, \\sigma_Y^2)$ population. The random variable $F= (S_X^2 / \\sigma_X^2)/ (S_Y^2/\\sigma_Y^2)$ has Snedecr's F distribution with n-1 and m-1 degrees of freedom. Equivalently, the random varibale F has the F distribution with p and q degrees of freedom if it has pdf.\n",
        "\n",
        "$$ f_{f}(x)  = \\frac{\\Gamma(\\frac{p+q}{2})}{\\Gamma(\\frac{p}{2}) \\Gamma(\\frac{q}{2})} (\\frac{p}{1})^{p/2} \\frac{x^{p/2}-1}{[1+(p/q)x]^{(p+q)/2}}$$\n",
        "- $0< x < \\infty$\n",
        "\n",
        "\n",
        "Example 5.3.7. COntinuaton of Ex 5.3.5.\n",
        "\n",
        "\n",
        "Theorem: 5.3.8.\n",
        "1. if $X \\sim F_{p,q}$ then $1/X \\sim F_{q,p}$  that is, the reciprocal of an F random variables is again an F random variable\n",
        "2. If $X \\sim t_q$ then $X^2 \\sim_{1,q}$\n",
        "3. if $X \\sim F_{p,q}$ then $(p/q)X/ (1+(p/q)X) \\sim beta(p/2, q/2)$\n",
        "\n"
      ],
      "metadata": {
        "id": "t8e8GoQcOM4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4. Order Statistics\n",
        "\n",
        "5.4.1. The order statistics of a random sample $X_+1...X_n$ are the sample values placed in ascending order. They are denoted by $X_{(1)} ... X_{(n)}$\n",
        "\n",
        "5.4.2. The notation {b}, whne appearing in a subscript , is defined to be the number b rounded to the nearest integer in the usual way. More precisely, if i is an integer and $i-0.5 =< b =< i+-0.5$ then {b}=i.\n",
        "\n",
        "Theorem: 5.4.3. Let $X_1..X_n$ be a random sample from a discrete distribution with pmf $f_X(x_i) = p_i$ where $x_1 < x_2 < ... $ are the posible values of X in ascending order:\n",
        "Define:\n",
        "- $P_0 = 0$\n",
        "- $P_1 = p_1$\n",
        "- $P_2 = p_1 + p_2$\n",
        "- ....\n",
        "- $P_i = p_1 + p_2 + ... +p_i$\n",
        "\n",
        "Let $X_{(1)} ... X_{(n)}$ denote the order statistics from the sample. Then:\n",
        "\n",
        "$$P(X_{(j)} < x_i) \\sum_{k=1}^n \\binom{n}{k} P_i^k (1-P_i)^{n-k}$$\n",
        "\n",
        "and\n",
        "\n",
        "$$P(X_{(j)}= x_i) \\ sum_{k=j}^n \\binom{n}{k} [P_i^k (1-P_i)^{n-k} - P_{i-1}^k (1-P_{i-1}^{n-k})]$$\n",
        "\n",
        "Proof:\n",
        "\n",
        "Theorem: 5.4.4. Let $X_{(1)} ... X_{(n)}$ denote the order statistic of a random sample, $X_1...X_n$ from a continuous population with cdf $F_X(x)$ and pdf $f_X(x)$. Then the pdf of $X_{(j)}$ is:\n",
        "\n",
        "$$f_{X_{(j)}} (x) = \\frac{n!}{(j-1)! (n-j)!} f_X(x) [F_X(x)]^{j-1} [1- F_X(x)]^{n-j}$$\n",
        "\n",
        "Proof:\n",
        "\n",
        "Ex: 5.4.5. Uniform order statistic pdf\n",
        "\n",
        "\n",
        "\n",
        "Theorem:5.4.6.\n",
        "\n",
        "Ex: 5.4.7. Distirbution o the midrange and range\n",
        "\n"
      ],
      "metadata": {
        "id": "7fmGJ3n7OM6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5 Convergence Concept\n",
        "### 5.5.1. Convergence in Probability:\n",
        "\n",
        "5.5.1. A sequence of random variables $X_1,X_2....$ converges in probability to random variable X if for every $\\epsilon >0$\n",
        "\n",
        "$$lim_{n-> \\infty} P(|X_n - X| >= \\epsilon) = 0$$\n",
        "\n",
        "or , equvalently\n",
        "\n",
        "$$lim_{n->\\infty} P(|X_n - X| < \\epsilon)=1$$\n",
        "\n",
        "Theorem 5.5.2. Weak Law of Large Numbers: Let $X_1,X_2 ...$ be iid random variables with $E[X_i]= \\mu$ and $Var[X_i]= \\sigma^2 < \\infty$. Define $\\bar{X_n} = (1/n) \\sum_{i=1}^n X_i$. Then, for every $\\epsilon >0$\n",
        "\n",
        "$$lim_{n-> \\infty} P(|\\bar{X_n} - \\mu| < \\epsilon)=1$$\n",
        "- that is $\\bar{X_n}$ converges in probability to $\\mu$\n",
        "\n",
        "Proof\n",
        "\n",
        "Ex: 5.5.3. Consitency of $X^2$\n",
        "\n",
        "Theorem: 5.5.4.  SUppose $X_1 X_2...$ converges in probability to a random varibale X and that h is continuous function. Then $h(X_1), h(X_2)$ .. converges in probability to $h(X)$\n",
        "\n",
        "Ex: 5.5.5 consitency of S\n",
        "\n",
        "Define 5.5.6. A sequence o random variables $X_1,X_2...$ converges almost surely to a random variable X if , for every $ϵ > 0$\n",
        "\n",
        "$$P(lim_{n-> \\infty} |X_n - X| < \\epsilon) = 1$$\n",
        "\n",
        "Ex: 5.5.8. COnvergence in probability , not almost surely\n",
        "\n",
        "Theorem: 5.,5.9. Strong Law of Large Numbers: Let $X_1, X_2 ...$ be iid random variables with $E[X_i] = \\mu$ and $Var[X_i]=\\sigma^2 < \\infty$ and define $\\bar{X_n} = (1/n) \\sum_{i=1}^n X_i$. Then , for every $\\epsilon > 0$:\n",
        "$$P(lim_{n-> \\infty} |\\bar{X_n} - \\mu| < \\epsilon)= 1$$\n",
        "\n",
        "that is $\\bar{X_n}$ conveges almost surely to $\\mu$\n",
        "\n",
        "### 5.5.3. Convergence in Distribution:\n",
        "\n",
        "5.5.10. A sequence of randoam variaables $X_1.X_2...$ converges in dsitreibution to a random variable X if:\n",
        "\n",
        "$$lim_{n-> \\infty} F_{X_n} (x)= F_X (x)$$\n",
        "\n",
        "at all points x where $F_X(x)$ is continuous\n",
        "\n",
        "Ex: 5.5.11. Maximum of uniforms\n",
        "\n",
        "Theorem: 5.5.12. If the sequence of random variables $X_1,X_2...$ converges in probability to a random variables X , the sequence also converges in distribution to X\n",
        "\n",
        "Theorem: 5.5.13. The sequence of random variables $X_1,X_2 ...$ converges in probability  to a constant $\\mu$ if and only if the sequence also converges in distribution to $\\mu$. That is,m the statement:\n",
        "$$P(|X_n - \\mu| > \\epsilon) => 0 $$ for every $\\epsilon>0$\n",
        "\n",
        "is equvalent:\n",
        "$$P(X_n =< x)$$\n",
        "- 0 if $x < \\mu$\n",
        "- 1 if $x > \\mu$\n",
        "\n",
        "\n",
        "Theorem: 5.5.14. Central Limit theorem: Let $X_1,X_2...$ be a sequence of iid random variables whose mgf-s exist in neighborhood of 0 (that is, $M_{X_i}(t)$ exist for $|t| < h$ for some positive h). Let $E[X_i]= \\mu$ and $Var[X_i]= \\sigma^2 >0$. Both $\\mu$ and $\\sigma^2$  . (Both $\\mu$ and $\\sigma^2$ are finite since the mgf exists). Define $\\bar{X_n}=(1/n)\\sum_{i=1}^n$. Let $G_n (x)$ denote the cdf of $\\sqrt{n} (\\bar{X_n} -\\mu) /\\sigma$. Then,m for any x, $-\\infty < x < \\infty$\n",
        "\n",
        "$$ $lim_{n-> \\infty} G(n) = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-y^2/2} dy$\n",
        "\n",
        "that is $\\sqrt{n} (\\bar{X_n} - \\mu)/\\sigma$ has a limiting standard normal distribution.\n",
        "\n",
        "Proof theorem 5.5.14.\n",
        "\n",
        "Theorem: 5.5.15. Strong form of the Central Limit Theorem: Let $X_1, X_2...$ be a sequence of iid random variable with $E[X_i]= \\mu$ and $Var[X_i]= \\sigma^2 < \\infty$. Define $\\bar{X_n} = (1/n) \\sum_{i=1}^n X_i$. Let $G_n (x)$ denote the cdf of $\\sqrt{n} (\\bar{X_n} -\\mu) / \\sigma$.\n",
        "\n",
        "Then , for any $x, -\\infty < x < \\infty$\n",
        "\n",
        "$$ lim_{n-> \\infty} G_n (x) = \\int_{-\\infty}^{x} \\frac{1}{\\sqrt{2\\pi}} e^{-y^2/2} dy$$\n",
        "\n",
        "that is $\\sqrt{x} (\\bar{X_n}- \\mu) /\\sigma$ has a limiting standard normal distribution\n",
        "\n",
        "Ex: 5.5.16 - Normal approximation to the negative binomial\n",
        "\n",
        "Theorem 5.5.17. Slutsky Theorem: If $X_n -> X$ is distribution and $Y_n -> a$ a constant, in probability , then\n",
        "- $Y_n X_n -> aX$ in distribution\n",
        "- $X_n + Y_n-> X + a$ in distribution\n",
        "\n",
        "Ex: 5.5.18 Normal approximation with estimated variance\n",
        "\n",
        "### 5.5.4. The Delta Method:\n",
        "\n",
        "Ex: 5.5.19 Estimating the odds\n",
        "\n",
        "5.5.20 If a function $g(x)$ has derivative of order r, that is $g^{(r)} (x) = \\frac{d^r}{dx^r} g(x)$ exist, then for any constant a, the Taylor polynomial of order r about a is:\n",
        "\n",
        "$$T_r (x) = \\sum_{i=0}^r \\frac{g^{(i)} (a)}{i!} (x-a)^i$$\n",
        "\n",
        "Theorem 5.5.21: Taylor:\n",
        "\n",
        "If $g^{(r)} (a) = \\frac{d^r}{dx^r} g(x) |_{x=a}$\n",
        " exist , then:\n",
        " $$lim_{x-> a} \\frac{g(x) - T_r (x)}{(x-a)^r}=0$$\n",
        "\n",
        " Ex: 5.5.22: Continuation of ex 5.5.19\n",
        "\n",
        " Ex: 5.5.23. Approximate mean and variance\n",
        "\n",
        " Theorem: 5.5.24. Melta method: Let $Y_n$ be a sequence of random variables that satisfies $\\sqrt{n} (Y_n - \\theta) -> N(0, \\sigma^2)$ is distribution. For a given function g and a specific value of $\\theta$, suppose that $q`(\\theta)$ exists and is not 0. Then:\n",
        "\n",
        " $$ \\sqrt{n} [g(Y_n) - g(\\theta)] -> N(0, \\sigma^2 [g`(\\theta)]^2)$$ in distibution\n",
        "\n",
        " Proof:\n",
        "\n",
        " Ex: 5.5.25\n",
        "\n",
        " Theorem: 5.5.26 Second order Delta Method: Let $Y_n$ be a sequence of random variables that satisfies $\\sqrt{n}(Y_n - \\theta) -> N(\\mu, \\sigma^2)$ in distributiopn. For a given function g and specific value of $\\theta$ , suppose that $g` (\\theta) = 0$ and $g``(\\theta)$ exist and is not 0. Then:\n",
        " $$N[g(Y_n) - g(\\theta) -> \\sigma^2 \\frac{g``(\\theta)}{x} \\chi_1^2 ]$$ in distribution\n",
        "\n",
        " Ex: 5.5.27 Moments of a ratio estimator\n",
        "\n",
        " Theorem 5.5.28 Multivariate Delta Method\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "uTnDMUHzOM8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.6. Generating a Random Sample\n",
        "\n",
        "Ex: 5.6.1. Exponential lfetime !!!!!!!!!\n",
        "\n",
        "Ex: 5.6.2. Continuation of ex 5.6.1.\n",
        "\n",
        "### 5.6.1. Direct Metods:\n",
        "\n",
        "ex: 5.6.3. Probabiliy Integral Transform\n",
        "\n",
        "ex: 5.6.4. Box Muller algorithm\n",
        "\n",
        "ex: 5.6.5 Binomial random variabale generation\n",
        "\n",
        "ex: 5.6.6. Distribution of the Poisson variance\n",
        "\n",
        "### 5.6.2. Indirect methods:\n",
        "\n",
        "ex: Beta random variable generation\n",
        "\n",
        "\n",
        "\n",
        "### 5.6.3. The Accept /Reject Algorithm:\n",
        "Theorem 5.6.8. Let $Y \\sim f_Y (y)$ and $V \\sim f_V(V)$ where $f_Y$ and $f_V$ have common support with:\n",
        "\n",
        "$$$M = sup_y f_Y(y) / f_V(y) < \\infty$\n",
        "\n",
        "to genertate a random variable $Y \\sim f_Y$\n",
        "\n",
        "- generate $U \\sim uniform(0,1), V \\sim f_V$ independent\n",
        "- if $U < \\frac{1}{M} f_Y(V) / f_V (V)$, set $Y=V$ otherwise , return to step (a)\n",
        "\n",
        "Proof:\n",
        "\n",
        "Ex: 5.6.9. Betta random variable generation\n",
        "\n",
        "### METROPOLIS ALGORTHM: Let $Y \\sim f_Y(y)$ and $V \\sim f_V (v)$ where $f_Y$ and $f_V$ have common support. To genertate $Y \\sim f_Y$\n",
        "\n",
        "0. Generate $V \\sim f_V$ . set $Z_0=V$ for i=-1,2,...\n",
        "\n",
        "1. Generate $U_i \\sim uniform(0,1), V_i \\sim f_V$ and calculate:\n",
        "\n",
        "$$\\rho_i = min (\\frac{f_Y (V_i)}{f_V (V_i)}  . \\frac{f_V(Z_{i-1})}{f_Y (Z_{i-1})}, 1)$$\n",
        "\n",
        "2. Set:\n",
        "$Z_i$\n",
        "- $V_i - if U_i =< \\rho_i$\n",
        "- $Z_{i-1}$ if $U_i > \\rho_i$\n",
        "\n",
        "Then , as $i -> \\infty$, $Z_i$ converges to Y in distribution\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yLuR6m1EOM--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W5G2505gONBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vo1dQSZ7ONDK"
      }
    }
  ]
}