{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpQ7mVjErcz3Jp2VgMp2DW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikolayLenkovNikolaev/Inferential_Statistics/blob/main/11_Analysis_of_Variance_and_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11.1. Introduction\n",
        "\n",
        "The function that gives E[Y] as a function of x is called the population regression function.\n",
        "\n",
        "## 11.2. Oneway Analysis of Variance\n",
        "\n",
        "$$11.2.1.---- Y_{i,j} = \\theta_i + ϵ_{i,j} $$\n",
        "- i = 1,..., k\n",
        "- $j= 1....n_i$\n",
        "- $\\theta_i$ are unknown parameteres\n",
        "- $\\epsilon_{ij}$ - error random variables\n",
        "\n",
        "ex: 11.2.1. Oneway ANOVA\n",
        "\n",
        "- schematically the data $y_{ij}$\n",
        "\n",
        "- $E[\\epsilon_{i,j}]= 0$\n",
        "\n",
        "- $E[Y_{ij}]= \\theta_i$\n",
        "  - $\\theta$ is the means of $Y_{ij}$\n",
        "  - $\\theta_i$ - tretment means\n",
        "\n",
        "ALternative model - overparametrized model\n",
        "$$11.2.2 ---Y_{ij} = \\mu + \\tau_i \\epsilon_{ij}$$\n",
        "- i=1...k\n",
        "- j=1....n_i\n",
        "- $E[\\epsilon_{ij}]= 0$ > $E[Y_{ij}]= \\mu +\\tau$\n",
        "  - $\\mu$ - grand mean\n",
        "  - $\\tau_i$ - unique effect due to treatment i-th, the deviation from the mean level that is caused by the treatment\n",
        "  - we cannot estimate both separately, because there are problems with *identifiability*\n",
        "\n",
        "11.2.2. A parameter $\\theta$ for a family of distribution $\\{ f(x|\\theta): \\theta \\in \\Theta \\}$ is identifiable if distinct values of $\\theta$ correspond to distinct pdf or pmfs. That is . if $\\theta \\ne \\theta`$ then $f(x|\\theta)$ is not the same function of x as $f(x|\\theta`)$\n",
        "\n",
        "11.2.2. k+1 parameters $(\\mu, \\tau_1 ...\\tau_k)$, only one means-k, E{$Y_{ij}$] i=1...k\n",
        "\n",
        "11.2.1= cell means model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o0ib06gjQYsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.2.1. Model and DIstribution Assumptions\n",
        "\n",
        "- $E[\\epsilon_{ij}] = 0$\n",
        "- $Var[\\epsilon_{ij}] < ∞$\n",
        "\n",
        "#### Oneway ANOVA assumptions:\n",
        "- $$Y_{ij}= \\theta_i + \\epsilon_{ij}$$\n",
        "\n",
        "i = $E[\\epsilon_{ij}]= 0, Var[\\epsilon_{ij}]= \\sigma_i^2 < \\infty$ for all i,j. $Cov(\\epsilon_{ij}, \\epsilon_{i`,j`})= 0$ for all i,i`,j,j` unless i=i` and j=j`\n",
        "ii - The $\\epsilon_{ij}$ are independent and normally distribuited - normal errros\n",
        "iii - $\\sigma_i^2 = \\sigma^2$ for all i- equality of variance , also known as homoscedasticity\n",
        "\n",
        "### 11.2.2. The Classic ANOVA Hypothesis\n",
        "\n",
        "The classic ANOVA test is a test of the null hypothesis\n",
        "$H_0: \\theta_1 = \\theta_2 =...= \\theta_k$ vs $\\theta+i \\ne \\theta_j$\n",
        "\n",
        "11.2.4. Let $t=(t_1...t_k)$ be a set of variables, either parameters or statistics, and let $a= (a_1...a_k)$ be known constants/ The function\n",
        "$$\\sum_{i=1}^k  a_i.t_i$$\n",
        "is called a linear combination of the $t_i$s. If, furthermore $\\sum a_i = 0$ is called contrast\n",
        "\n",
        "Contrtasts are important because they can be used to compare treatment means. For ee. if we have means $\\theta_1 ...\\theta_k$ and constants a= (1,-1,0...0) then:\n",
        "\n",
        "$$\\sum_{i=1}^k a_i.\\theta_i = \\theta_1 - \\theta_2$$ is a contrast that compares $\\theta_1$ to $\\theta_2$ ex: 11.10\n",
        "\n",
        "\n",
        "Theorem: 11.2.5. Let $\\theta=(\\theta_1...\\theta_k)$ be arbitrary parameters. Then $$\\theta_1 = \\theta_2 = ...=\\theta_k <=> \\sum_{i=1}^k a_i \\theta_i = 0$$  for all $a \\in \\mathcal{A}$ where A is the set of constants satisfying $\\mathcal{A} =\\{ a= (a_1...a_k)\\}$ that is all contrast must satisfy $\\sum a_i.\\theta_i=0$\n",
        "\n",
        "Proof:\n",
        "\n"
      ],
      "metadata": {
        "id": "vm_1RlwfQYvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.2.3. Inference Regarding Linear COmbination of Means\n",
        "\n",
        "$$ Y_{ij} \\sim N(\\theta_i, \\sigma^2)$$\n",
        "\n",
        "Yherefore:\n",
        "\n",
        "$$\\bar{Y} = \\frac{1}{n_i} \\sum_{j=1}^{n_i} Y_{ij} \\sim N(\\theta_i, \\sigma^2/n_i)$$\n",
        "\n"
      ],
      "metadata": {
        "id": "HJicKoujQYxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.2.4. The ANOVA F -test\n",
        "\n"
      ],
      "metadata": {
        "id": "w0aFviojQY0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eBMqyP1zQY2S"
      }
    }
  ]
}